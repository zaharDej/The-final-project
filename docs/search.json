[
  {
    "objectID": "step1_ocr.html",
    "href": "step1_ocr.html",
    "title": "Шаг 1-2: Извлечение текста из PDF",
    "section": "",
    "text": "Источник корпуса: 15 PDF-файлов рассказов Эдгара По.\nПапка: source_docs/\nИнструменты: pdftools для извлечения текста, magick для предобработки изображений (улучшение контраста, удаление перекоса).\nlibrary(pdftools)  \nlibrary(magick)    \n\nsource_dir &lt;- \"Files/source_docs/\"\nraw_dir &lt;- \"Files/raw_text/\"\nexamples_dir &lt;- \"results/ocr_examples/\"\n\ndir.create(raw_dir, showWarnings = FALSE)\ndir.create(\"Files/results\", showWarnings = FALSE)\ndir.create(examples_dir, showWarnings = FALSE)\n\npdf_files &lt;- list.files(source_dir, pattern = \".pdf$\", full.names = TRUE, ignore.case = TRUE)\n\ncat(\"Найдено файлов:\", length(pdf_files), \"\\n\\n\")\n\nfor (pdf_path in pdf_files) {\n  name &lt;- tools::file_path_sans_ext(basename(pdf_path))\n  cat(\"Обрабатываю:\", name, \".pdf\\n\")\n  \n  # Извлекаем текст напрямую\n  pages_text &lt;- pdf_text(pdf_path)\n  full_text &lt;- paste(pages_text, collapse = \"\\n\")\n  \n  # Сохраняем сырой текст\n  writeLines(full_text, file.path(raw_dir, paste0(name, \"_raw.txt\")))\n  cat(\"  Сохранён _raw.txt\\n\")\nПример сырого текста \nПример предобработки первой страницы (The Fall of the House of Usher):"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Главная",
    "section": "",
    "text": "1 Итоговый проект: Вариант 1\nКомплексный анализ корпуса с визуализацией\nАвтор: Дей Захар Дмитриевич\n\n\n\nЭдгар Аллан По\n\n\n\n“All that we see or seem is but a dream within a dream.”\n— Edgar Allan Poe\n\nКорпус состоит из 15 рассказов Эдгара Аллана По (английский язык):\nThe Fall of the House of Usher The Black Cat The Masque of the Red Death The Cask of Amontillado The Pit and the Pendulum The Purloined Letter The Facts in the Case of M. Valdemar The Premature Burial The Assignation Mesmeric Revelation The Imp of the Perverse The Island of the Fay Silence—A Fable The Thousand-and-Second Tale of Scheherazade Von Kempelen and His Discovery\n\n1.0.1 Что сделано:\n\nИзвлечение текста из PDF (pdftools + предобработка изображений magick)\nОчистка текста регулярными выражениями\nЛемматизация и частотный словарь (udpipe)\nПоиск коллокаций (quanteda) и визуализация (ggplot2, ggraph)\n\n\n\n1.0.2 Ключевые результаты\n\n\n\nТоп-30 лемм\n\n\n\n\n\n\n\n Интерпретация: доминируют темы смерти, упадка, сверхъестественного. Коллокации прямо отсылают к названиям рассказов (“red death”, “house usher”, “black cat”).\n→ Используйте меню сверху для детального просмотра каждого шага."
  },
  {
    "objectID": "poe_analysis.html",
    "href": "poe_analysis.html",
    "title": "Шаги 2–4: Очистка текста, лемматизация, коллокации и визуализация",
    "section": "",
    "text": "1 Шаг 2: Очистка текста регулярными выражениями\nПосле извлечения текст содержал артефакты форматирования PDF (лишние переносы, тире, номера страниц). Очистка проводилась с помощью пакета stringr.\nЗадачи очистки: - Исправление типичных ошибок (например, “rn” → “m”, одинокая “l” → “I”) - Приведение к нижнему регистру - Удаление пунктуации, цифр и слишком коротких слов - Нормализация пробелов\nРезультат шага 1\n\nПолучено 15 файлов с сырым извлечённым текстом.\nПапка с результатами: raw_text/\nОбщий объём корпуса после очистки: ~293 тыс. символов.\nДемонстрация предобработки изображений показывает, как можно работать с полностью отсканированными источниками (если бы текстовый слой в PDF отсутствовал).\n\n\n\n2 Шаг 3: Лемматизация и лемматизированный частотный словарь\nЛемматизация и POS-тэггинг с помощьюudpipe. Учитывались только содержательные части речи (NOUN, ADJ, VERB), минимальная частота — 5.\n\n\nПоказать код\n# объединяем всё в один корпус\nclean_files &lt;- list.files(clean_dir, pattern = \"_clean\\\\.txt$\", full.names = TRUE)\n\ncorpus &lt;- \"\"   # сюда собираем весь текст\nfor (file in clean_files) {\n  text &lt;- readLines(file, encoding = \"UTF-8\")\n  corpus &lt;- paste(corpus, paste(text, collapse = \" \"))\n}\ncat(\"Корпус готов. Символов:\", nchar(corpus), \"\\n\\n\")\n\n# Загружаем модель udpipe\nmodel_path &lt;- \"english-ewt-ud-2.5-191206.udpipe\"\nud_model &lt;- udpipe_load_model(model_path)\n\n# лемматизация\nannotated &lt;- udpipe_annotate(ud_model, x = corpus)\nannotated &lt;- as.data.frame(annotated)\n\n# Частотный словарь лемм (существительные, прилагательные, глаголы)\ncat(\"Словарь\\n\")\nfreq_dict &lt;- annotated[annotated$upos %in% c(\"NOUN\", \"ADJ\", \"VERB\"), ]\nfreq_dict &lt;- table(freq_dict$lemma)\nfreq_dict &lt;- sort(freq_dict, decreasing = TRUE)\nfreq_dict &lt;- freq_dict[freq_dict &gt;= 5]   # частота слов &gt;= 5\n\n# Сохраняем словарь в файл\nwrite.csv(as.data.frame(freq_dict), file.path(results_dir, \"frequency_dictionary.csv\"), row.names = TRUE)\ncat(\"Частотный словарь сохранён\\n\")\n\n\n\n\n3 Топ-30 наиболее частотных лемм\n\n\nПоказать код\n# Берём топ лемм\ntop_n &lt;- min(30, length(freq_dict))\n\ntop30_names &lt;- names(freq_dict)[1:top_n]\ntop30_counts &lt;- as.numeric(freq_dict[1:top_n])\n\n# Создаём датафрейм\ntop30_df &lt;- data.frame(\n  lemma = top30_names,\n  count = top30_counts,\n  stringsAsFactors = FALSE\n)\n\n# График\nggplot(top30_df, aes(x = reorder(lemma, count), y = count)) +\n  geom_col(fill = \"darkred\", width = 0.7) +\n  coord_flip() +\n  labs(title = \"Топ-30 лемм в корпусе Эдгара Аллана По\",\n       x = \"Лемма\", y = \"Частота\") +\n  theme_minimal(base_size = 12)\n\nggsave(file.path(results_dir, \"top30_lemmas.png\"), width = 12, height = 9, dpi = 300)\n\n\n Интерпретация: доминируют слова, связанные со смертью (death), домом (house), временем (time), глазами (eye), сердцем (heart) — типичные мотивы готической прозы По. # Шаг 4: Коллокации и визуализация\n\n\nПоказать код\n# Коллокации\ntoks &lt;- tokens(corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE)\n\ncolloc &lt;- textstat_collocations(toks, size = 2, min_count = 10)\ncolloc &lt;- colloc[order(colloc$count, decreasing = TRUE), ]\n\n# Берём топ-30\ntop_colloc &lt;- colloc[1:30, ]\n\nwrite.csv(top_colloc, file.path(results_dir, \"collocations.csv\"), row.names = FALSE)\n\ntop20_colloc &lt;- top_colloc[1:20, ]\nggplot(top20_colloc, aes(x = reorder(collocation, count), y = count)) +\n  geom_col(fill = \"darkblue\") +\n  coord_flip() +\n  labs(title = \"Топ-20 коллокаций в корпусе По\", x = \"Коллокация\", y = \"Частота\") +\n  theme_minimal()\nggsave(file.path(results_dir, \"top20_collocations_bar.png\"), width = 12, height = 8)\n\n\n Интерпретация: многие коллокации — прямые отсылки к названиям и ключевым образам рассказов: “red death”, “house usher”, “black cat”, “tell tale”, “fall house”.\n\n\n4 Сетевой график коллокаций\n\n\nПоказать код\n# Сетевой график коллокаций\nwords1 &lt;- character(nrow(top_colloc))\nwords2 &lt;- character(nrow(top_colloc))\nfor (i in 1:nrow(top_colloc)) {\n  parts &lt;- str_split(top_colloc$collocation[i], \" \")[[1]]\n  words1[i] &lt;- parts[1]\n  words2[i] &lt;- parts[2]\n}\ngraph_df &lt;- data.frame(word1 = words1, word2 = words2, weight = top_colloc$count)\n\ngraph &lt;- graph_from_data_frame(graph_df, directed = FALSE)\n\nggraph(graph, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = weight, edge_width = weight), colour = \"lightgray\") +\n  geom_node_point(color = \"white\", size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE, colour = \"white\") +\n  labs(title = \"Сеть топ-30 коллокаций\", colour = \"white\") +\n  theme_void()\nggsave(file.path(results_dir, \"collocations_network.png\"), width = 12, height = 10)\ncat(\"Готово\\n\")\n\n\n\n\n\nСеть коллокаций (топ-30)"
  },
  {
    "objectID": "step1_ocr.html#результат-шага-1",
    "href": "step1_ocr.html#результат-шага-1",
    "title": "Шаг 1-2: Извлечение текста из PDF",
    "section": "",
    "text": "Получено 15 файлов с сырым извлечённым текстом. Пример сырого текста \nПапка с результатами: raw_text/\nОбщий объём корпуса после очистки: ~293 тыс. символов.\nДемонстрация предобработки изображений показывает, как можно работать с полностью отсканированными источниками (если бы текстовый слой в PDF отсутствовал).\nПример очищенного текста"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Выводы и информация о проекте",
    "section": "",
    "text": "1 Основные выводы анализа\nКорпус из 15 рассказов Эдгара Аллана По позволил выявить ключевые лексические особенности его прозы:\n\nДоминирующие темы: смерть (death), дом (house), время (time), глаз (eye), сердце (heart), страх (fear), тьма (dark). Эти леммы отражают готическую атмосферу упадка, безумия и сверхъестественного.\nЗначимые коллокации: “red death”, “house usher”, “black cat”, “tell tale”, “fall house” — прямые отсылки к названиям и центральным образам произведений.\nОбщая картина: в тексте встречается много словоформ с негативной коннотацией. Пейоративы, вполне типичные для готической прозы, усиливают саспенс и сюжетное напряжение, ознаменующее собой триумф безысходности – тематически характерный для По.\n\nАнализ подтверждает, что По мастерски использует повторяющиеся мотивы для построения напряжения.\n\n\n\n\nФинальная сеть коллокаций — визуализация ключевых связей\n\n\n\n\n\n2 Техническая реализация проекта\nВариант 1 выполнен полностью и с превышением требований задания:\n\n\n\n\n\n\n\nПункт задания\nВыполнение\n\n\n\n\n1. Извлечение текста (OCR с tesseract или из PDF)\nИзвлечение из 15 PDF (pdftools) + демонстрация предобработки изображений (magick)\n\n\n2. Очистка регулярными выражениями\nПолная очистка: исправление артефактов, нормализация, удаление шума\n\n\n3. Лемматизированный частотный словарь (udpipe)\nПостроен с фильтрацией по частям речи и частоте\n\n\n4. Коллокации и визуализация\nТри вида: барплот, таблица, сетевой график (ggraph)\n\n\n\nКорпус: &gt;10 документов (15 PDF, общественное достояние).\nОбщий объём: ~293 тыс. символов после очистки.\nИнструменты: R, pdftools, magick, stringr, udpipe, quanteda, ggplot2, ggraph.\nПроект полностью воспроизводим: достаточно запустить скрипты по порядку: 1. scripts/01.R 2. scripts/02.R 3. scripts/03.R\n\n\n3 Заключение\nРабота с текстами Эдгара По показала, что количественные характеристики для эмпирического материала могут быть легко выделены при помощи простых методов цифрового анализа с целью выявления особенностей идиостиля писателя или поэта. Полученные результаты не только соответствуют канону литературных исследований творчества По, но и демонстрируют практическую ценность инструментов обработки естественного языка.\nСпасибо за интересный курс и возможность применить знания на примере корпуса текстов мировой художественной прозы!\n\n“The boundaries which divide Life from Death are at best shadowy and vague. Who shall say where the one ends, and where the other begins?”\n— Edgar Allan Poe, The Premature Burial\n\n\n\n\n\nТоп-30 лемм — лексический портрет корпуса"
  },
  {
    "objectID": "step2-4_analysis.html",
    "href": "step2-4_analysis.html",
    "title": "Шаги 3–4: Лемматизация и построение частотного словаря",
    "section": "",
    "text": "1 Шаг 3: Лемматизация и лемматизированный частотный словарь\nТекст корпуса объединяется из очищенных файлов, затем проводится лемматизация и POS-тэггинг с помощью модели udpipe (английская модель english-ewt).\nВ частотный словарь включаются только леммы со следующими частями речи:\n\nNOUN (существительные)\nADJ (прилагательные)\nVERB (глаголы)\n\nМинимальная частота — 5 вхождений.\nclean_dir &lt;- \"Files/clean_text/\"\nresults_dir &lt;- \"results/\"\ndir.create(results_dir, showWarnings = FALSE, recursive = TRUE)\n\n# объединяем всё в один корпус\nclean_files &lt;- list.files(clean_dir, pattern = \"_clean\\\\.txt$\", full.names = TRUE)\ncorpus &lt;- \"\" \nfor (file in clean_files) {\n  text &lt;- readLines(file, encoding = \"UTF-8\")\n  corpus &lt;- paste(corpus, paste(text, collapse = \" \"))\n}\ncat(\"Корпус готов. Символов:\", nchar(corpus), \"\\n\\n\")\n\n\nmodel_path &lt;- \"english-ewt-ud-2.5-191206.udpipe\"\nud_model &lt;- udpipe_load_model(model_path)\n\n# лемматизация + POS\nannotated &lt;- udpipe_annotate(ud_model, x = corpus)\nannotated &lt;- as.data.frame(annotated)\n\n# Частотный словарь лемм (NOUN, ADJ, VERB)\ncat(\"Словарь\\n\")\nfreq_dict &lt;- annotated |&gt;\n  filter(upos %in% c(\"NOUN\", \"ADJ\", \"VERB\")) |&gt;\n  count(lemma, sort = TRUE) |&gt;\n  filter(n &gt;= 5)\n\nwrite.csv(freq_dict, file.path(results_dir, \"frequency_dictionary.csv\"), row.names = FALSE)\ncat(\"Частотный словарь сохранён\\n\")\n\n\n2 Топ-30 наиболее частотных лемм\n# Топ-30 лемм\ntop30_df &lt;- head(freq_dict, 30)\n\nggplot(top30_df, aes(x = reorder(lemma, n), y = n)) +\n  geom_col(fill = \"darkred\", width = 0.7) +\n  coord_flip() +\n  labs(title = \"Топ-30 лемм в корпусе Эдгара Аллана По\",\n       x = \"Лемма\", y = \"Частота\") +\n  theme_minimal(base_size = 12)\n\nggsave(file.path(results_dir, \"top30_lemmas.png\"), width = 12, height = 9, dpi = 300)\n\ncat(\"Извлекаем содержательные коллокации (ADJ+NOUN, NOUN+NOUN и т.д.)\\n\")\n\ncolloc &lt;- keywords_collocation(annotated,\n                               term = \"lemma\",             \n                               group = c(\"doc_id\", \"sentence_id\"),\n                               ngram_max = 3,               \n                               n_min = 5)                   \n\ncolloc &lt;- colloc |&gt;\n  arrange(desc(pmi)) |&gt;                  \n  head(50)                                 \n\n# Сохраняем полный список\nwrite.csv(colloc, file.path(results_dir, \"meaningful_collocations.csv\"), row.names = FALSE)\n Интерпретация: среди лидеров — слова, тесно связанные с ключевыми мотивами По: death (смерть), house (дом), time (время), eye(глаз), heart (сердце), man (человек), room (комната). Эти лексемы отражают характерные темы готической прозы автора: страх, безумие, смерть, замкнутое пространство.\n\n\n3 Шаг 4: Извлечение содержательных коллокаций и визуализация\nДля получения осмысленных словосочетаний используется функция keywords_collocation() из пакета udpipe. Она автоматически выделяет статистически значимые n-граммы (биграммы и триграммы), опираясь на POS-теги и меру ассоциации PMI (Pointwise Mutual Information).\nТаким образом удаётся избавиться от банальных служебных сочетаний («and the», «of the») и сосредоточиться на тематически важных словосочетаниях.\nВот, что было до.  \ntop20_colloc &lt;- head(colloc, 20)\ntop20_colloc$collocation &lt;- factor(top20_colloc$keyword, levels = rev(top20_colloc$keyword))\n\nggplot(top20_colloc, aes(x = collocation, y = freq)) +\n  geom_col(fill = \"darkred\", width = 0.7) +\n  coord_flip() +\n  labs(title = \"Топ-20 содержательных коллокаций в корпусе По\",\n       subtitle = \"(прилагательное + существительное и др., без служебных слов)\",\n       x = \"Коллокация\", y = \"Частота\") +\n  theme_minimal(base_size = 12)\n\nggsave(file.path(results_dir, \"top20_meaningful_collocations_bar.png\"), width = 12, height = 8, dpi = 300)\nПосле.  \nИнтерпретация: Здесь видны прямые отсылки к знаменитым произведениям и центральным образам:\n\n«red death» — «Маска Красной смерти»\n«black cat» — «Чёрный кот»\n«tell tale» и «tale heart» — «Сердце-обличитель»\n«house usher» и «fall house» — «Падение дома Ашеров»\n«old man» — частый персонаж в рассказах По\n\nТакие коллокации гораздо лучше раскрывают лексико-тематическое своеобразие корпуса, чем простые биграммы на сыром тексте.\n\n\n4 Сетевой график топ-30 коллокаций\n\n# Сетевой график (топ-30 коллокаций)\ntop30_colloc_net &lt;- head(colloc, 30)\n\ngraph_df &lt;- top30_colloc_net |&gt;\n  separate(keyword, into = c(\"word1\", \"word2\"), sep = \" \", extra = \"merge\") |&gt; \n  mutate(word2 = str_trim(word2)) |&gt;\n  select(word1, word2, weight = freq)\n\ngraph &lt;- graph_from_data_frame(graph_df, directed = FALSE)\n\nggraph(graph, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = weight, edge_width = weight),\n                 colour = \"black\", show.legend = FALSE) +\n  scale_edge_width(range = c(0.5, 4)) +\n  geom_node_point(color = \"darkred\", size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE, colour = \"black\", size = 4) +\n  labs(title = \"Сеть топ-30 содержательных коллокаций в корпусе Эдгара По\") +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    panel.background = element_rect(fill = \"white\", colour = NA),\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\nggsave(file.path(results_dir, \"meaningful_collocations_network.png\"), width = 12, height = 10, dpi = 300, bg = \"white\")\n\ncat(\"Готово!\\n\")\n Сетевой график позволяет увидеть связи между ключевыми словами: центральные узлы — это слова, наиболее часто входящие в значимые словосочетания (например, «death», «house», «man»). Толщина рёбер отражает частоту коллокации."
  },
  {
    "objectID": "step1_ocr.html#результат-шага-1---2",
    "href": "step1_ocr.html#результат-шага-1---2",
    "title": "Шаг 1-2: Извлечение текста из PDF",
    "section": "2.1 Результат шага 1 - 2",
    "text": "2.1 Результат шага 1 - 2\n\nПолучено 15 файлов с сырым извлечённым текстом.\nПапка с результатами: raw_text/\nОбщий объём корпуса после очистки: ~293 тыс. символов.\nДемонстрация предобработки изображений показывает, как можно работать с полностью отсканированными источниками (если бы текстовый слой в PDF отсутствовал).\nПроизведена очистка"
  }
]