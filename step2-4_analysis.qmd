---
title: "Шаги 3–4: Лемматизация и построение частотного словаря"
format: html
---

# Шаг 3: Лемматизация и лемматизированный частотный словарь
Текст корпуса объединяется из очищенных файлов, затем проводится лемматизация и POS-тэггинг с помощью модели `udpipe` (английская модель `english-ewt`).<br>

В частотный словарь включаются только леммы со следующими частями речи:

- NOUN (существительные)
- ADJ (прилагательные)
- VERB (глаголы)

Минимальная частота — 5 вхождений.

```r 
clean_dir <- "Files/clean_text/"
results_dir <- "results/"
dir.create(results_dir, showWarnings = FALSE, recursive = TRUE)

# объединяем всё в один корпус
clean_files <- list.files(clean_dir, pattern = "_clean\\.txt$", full.names = TRUE)
corpus <- "" 
for (file in clean_files) {
  text <- readLines(file, encoding = "UTF-8")
  corpus <- paste(corpus, paste(text, collapse = " "))
}
cat("Корпус готов. Символов:", nchar(corpus), "\n\n")


model_path <- "english-ewt-ud-2.5-191206.udpipe"
ud_model <- udpipe_load_model(model_path)

# лемматизация + POS
annotated <- udpipe_annotate(ud_model, x = corpus)
annotated <- as.data.frame(annotated)

# Частотный словарь лемм (NOUN, ADJ, VERB)
cat("Словарь\n")
freq_dict <- annotated |>
  filter(upos %in% c("NOUN", "ADJ", "VERB")) |>
  count(lemma, sort = TRUE) |>
  filter(n >= 5)

write.csv(freq_dict, file.path(results_dir, "frequency_dictionary.csv"), row.names = FALSE)
cat("Частотный словарь сохранён\n")
```
# Топ-30 наиболее частотных лемм


```r 
# Топ-30 лемм
top30_df <- head(freq_dict, 30)

ggplot(top30_df, aes(x = reorder(lemma, n), y = n)) +
  geom_col(fill = "darkred", width = 0.7) +
  coord_flip() +
  labs(title = "Топ-30 лемм в корпусе Эдгара Аллана По",
       x = "Лемма", y = "Частота") +
  theme_minimal(base_size = 12)

ggsave(file.path(results_dir, "top30_lemmas.png"), width = 12, height = 9, dpi = 300)

cat("Извлекаем содержательные коллокации (ADJ+NOUN, NOUN+NOUN и т.д.)\n")

colloc <- keywords_collocation(annotated,
                               term = "lemma",             
                               group = c("doc_id", "sentence_id"),
                               ngram_max = 3,               
                               n_min = 5)                   

colloc <- colloc |>
  arrange(desc(pmi)) |>                  
  head(50)                                 

# Сохраняем полный список
write.csv(colloc, file.path(results_dir, "meaningful_collocations.csv"), row.names = FALSE)

```
![Топ-30 лемм](results/top30_lemmas.png){width=100%}
**Интерпретация:** среди лидеров — слова, тесно связанные с ключевыми мотивами По: *death* (смерть), *house* (дом), *time* (время), *eye*(глаз), *heart* (сердце), *man* (человек), *room* (комната). Эти лексемы отражают характерные темы готической прозы автора: страх, безумие, смерть, замкнутое пространство.

# Шаг 4: Извлечение содержательных коллокаций и визуализация

Для получения осмысленных словосочетаний используется функция `keywords_collocation()` из пакета `udpipe`. Она автоматически выделяет статистически значимые n-граммы (биграммы и триграммы), опираясь на POS-теги и меру ассоциации PMI (Pointwise Mutual Information).

Таким образом удаётся избавиться от банальных служебных сочетаний («and the», «of the») и сосредоточиться на тематически важных словосочетаниях.

Вот, что было до. <br>
![Топ-20 коллокаций](results/top20_collocations_bar.png){width=95% fig-align="center"}

```r 
top20_colloc <- head(colloc, 20)
top20_colloc$collocation <- factor(top20_colloc$keyword, levels = rev(top20_colloc$keyword))

ggplot(top20_colloc, aes(x = collocation, y = freq)) +
  geom_col(fill = "darkred", width = 0.7) +
  coord_flip() +
  labs(title = "Топ-20 содержательных коллокаций в корпусе По",
       subtitle = "(прилагательное + существительное и др., без служебных слов)",
       x = "Коллокация", y = "Частота") +
  theme_minimal(base_size = 12)

ggsave(file.path(results_dir, "top20_meaningful_collocations_bar.png"), width = 12, height = 8, dpi = 300)

```
После. <br>
![](results/top20_meaningful_collocations_bar.png){width=100% fig-align="center"}


**Интерпретация**: 
Здесь видны прямые отсылки к знаменитым произведениям и центральным образам:

- «red death» — «Маска Красной смерти»
- «black cat» — «Чёрный кот»
- «tell tale» и «tale heart» — «Сердце-обличитель»
- «house usher» и «fall house» — «Падение дома Ашеров»
- «old man» — частый персонаж в рассказах По

Такие коллокации гораздо лучше раскрывают лексико-тематическое своеобразие корпуса, чем простые биграммы на сыром тексте.

# Сетевой график топ-30 коллокаций

```r

# Сетевой график (топ-30 коллокаций)
top30_colloc_net <- head(colloc, 30)

graph_df <- top30_colloc_net |>
  separate(keyword, into = c("word1", "word2"), sep = " ", extra = "merge") |> 
  mutate(word2 = str_trim(word2)) |>
  select(word1, word2, weight = freq)

graph <- graph_from_data_frame(graph_df, directed = FALSE)

ggraph(graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = weight, edge_width = weight),
                 colour = "black", show.legend = FALSE) +
  scale_edge_width(range = c(0.5, 4)) +
  geom_node_point(color = "darkred", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, colour = "black", size = 4) +
  labs(title = "Сеть топ-30 содержательных коллокаций в корпусе Эдгара По") +
  theme_void() +
  theme(
    plot.background = element_rect(fill = "white", colour = NA),
    panel.background = element_rect(fill = "white", colour = NA),
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold")
  )

ggsave(file.path(results_dir, "meaningful_collocations_network.png"), width = 12, height = 10, dpi = 300, bg = "white")

cat("Готово!\n")
```
![](results/meaningful_collocations_network.png){width=100% fig-align="center"}
Сетевой график позволяет увидеть связи между ключевыми словами: центральные узлы — это слова, наиболее часто входящие в значимые словосочетания (например, «death», «house», «man»). Толщина рёбер отражает частоту коллокации.